import numpy as np
import pandas as pd
import os, sys
from datetime import datetime

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report
from sklearn.utils.class_weight import compute_class_weight
import lightgbm as lgb

# ==========================
# ğŸ“‚ å‚æ•°è®¾ç½®
# ==========================
beat_up_dataset_path = "data/final_dataset/beat_dataset"
prev_window_size = 5   # log return çª—å£é•¿åº¦ï¼ˆ11ä¸ªä»·æ ¼â†’10ä¸ªlog returnï¼‰

# ==========================
# ğŸ“Š 1. åŠ è½½æ•°æ®
# ==========================
beat_up_dataset_list = []
for file in os.listdir(beat_up_dataset_path):
    beat_up_dataset = pd.read_parquet(os.path.join(beat_up_dataset_path, file), engine="pyarrow")
    beat_up_dataset_list.append(beat_up_dataset)

beat_up_dataset = pd.concat(beat_up_dataset_list).reset_index(drop=True)
print(f"âœ… number of total beat up data is {len(beat_up_dataset)}")

# ==========================
# ğŸ¯ 2. æ ‡ç­¾
# ==========================
y = beat_up_dataset["situation flag"].astype(int)
# y = y.replace({3: 1})

# ==========================
# ğŸ“ˆ 3. total price seq â†’ log return
# ==========================
def price_seq_to_log_returns(seq):
    prices = np.array(seq, dtype=np.float64)
    prices = prices[-prev_window_size:]
    returns = np.diff(np.log(prices))
    return returns

log_return_expanded = beat_up_dataset["price_prev"].apply(price_seq_to_log_returns)
log_return_df = pd.DataFrame(
    log_return_expanded.tolist(),
    columns=[f"log_return_day_{i}" for i in range(prev_window_size - 1)]
)

beat_up_dataset = pd.concat([beat_up_dataset.reset_index(drop=True), log_return_df], axis=1)

# ==========================
# ğŸ”§ 4. ç‰¹å¾åˆ—å‡†å¤‡
# ==========================
# è½¬æ¢ Surprise ä¸ºæ•°å€¼ç±»å‹
for col in [f"Surprise {i}" for i in range(8, 0, -1)]:
    beat_up_dataset[col] = pd.to_numeric(beat_up_dataset[col], errors='coerce').fillna(0)

numeric_features = [
    "EPS", "Surprise", "PE", "PE Change", "market cap", "%Px Chg"
] + [f"Surprise {i}" for i in range(8, 0, -1)] + [f"log_return_day_{i}" for i in range(prev_window_size - 1)]

categorical_features = ["Sector", "Quarter"]
feature_cols = numeric_features + categorical_features

X = beat_up_dataset[feature_cols]

# ==========================
# âœ‚ï¸ 5. åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
# ==========================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ==========================
# âš–ï¸ 6. è®¡ç®— class_weight
# ==========================
classes = np.unique(y_train)
weights = compute_class_weight(class_weight="balanced", classes=classes, y=y_train)
class_weight_dict = {cls: w for cls, w in zip(classes, weights)}
print("âœ… Computed class weights:", class_weight_dict)

# ==========================
# ğŸ§° 7. æ„å»ºé¢„å¤„ç†å™¨
# ==========================
numeric_transformer = Pipeline(steps=[
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ]
)

# ==========================
# ğŸŒ² 8. LightGBM æ¨¡å‹
# ==========================
model = lgb.LGBMClassifier(
    objective='multiclass',
    num_class=3,
    n_estimators=1000,          # âœ… å¢åŠ è¿­ä»£æ¬¡æ•°ï¼Œè®©æ¨¡å‹æœ‰æ›´å¤šå­¦ä¹ æœºä¼š
    learning_rate=0.02,         # âœ… é™ä½å­¦ä¹ ç‡ï¼Œè®©æ¯æ£µæ ‘å­¦å¾—æ›´ç»†
    num_leaves=20,             # âœ… å¢åŠ å¶å­æ•°ï¼Œæå‡æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼ˆé»˜è®¤31ï¼‰
    max_depth=-1,               # âœ… ä¸é™åˆ¶æ ‘æ·±åº¦ï¼Œè®©æ¨¡å‹è‡ªç”±åˆ†è£‚
    min_child_samples=5,        # âœ… å‡å°‘åˆ†è£‚æ‰€éœ€æœ€å°æ ·æœ¬æ•°ï¼Œé¿å…æ—©åœ
    min_split_gain=0.0,         # âœ… æ”¾å®½åˆ†è£‚å¢ç›Šé˜ˆå€¼ï¼Œç¡®ä¿ä¸ä¼šå¤ªæ—©åœæ­¢
    subsample=0.8,              # âœ… éšæœºé‡‡æ ·ï¼Œæå‡æ³›åŒ–
    colsample_bytree=0.8,       # âœ… ç‰¹å¾å­é‡‡æ ·ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ
    reg_alpha=0.1,              # âœ… L1 æ­£åˆ™åŒ–ï¼Œæ§åˆ¶å¤æ‚åº¦
    reg_lambda=0.1,             # âœ… L2 æ­£åˆ™åŒ–ï¼Œæ§åˆ¶å¤æ‚åº¦
    class_weight=class_weight_dict,  # âœ… ä¿æŒç±»åˆ«æƒé‡
    random_state=42,
    force_col_wise=True         # âœ… å¤šåˆ†ç±»å°æ•°æ®å»ºè®®å¼€å¯ï¼Œæé«˜æ•ˆç‡
)

clf = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", model)
])

# ==========================
# ğŸš€ 9. æ¨¡å‹è®­ç»ƒ
# ==========================
clf.fit(X_train, y_train)

# ==========================
# ğŸ“Š 10. é¢„æµ‹ä¸è¯„ä¼°
# ==========================
y_pred = clf.predict(X_test)
y_proba = clf.predict_proba(X_test)

# ğŸ“Š æ‰“å°æ ‡ç­¾å æ¯”ï¼ˆçœŸå®æ•°æ®åˆ†å¸ƒï¼‰
print("\nğŸ“Š Label distribution (train set):")
print(y_train.value_counts(normalize=True).sort_index().apply(lambda x: f"{x:.2%}"))

print("\nğŸ“Š Label distribution (test set):")
print(y_test.value_counts(normalize=True).sort_index().apply(lambda x: f"{x:.2%}"))


# ğŸ“Š æ‰“å°æƒé‡å æ¯”ï¼ˆä¸ç”¨äºæ¨¡å‹ï¼Œä»…åˆ†æç”¨ï¼‰
total_weight = sum(class_weight_dict.values())
weight_ratio = {cls: w / total_weight for cls, w in class_weight_dict.items()}

print("\nğŸ“Š Class weight ratios (not normalized for training, only for reference):")
for cls, ratio in weight_ratio.items():
    print(f"  Class {cls}: {ratio:.2%}")


print("âœ… Accuracy:", accuracy_score(y_test, y_pred))
print("âœ… F1 Score (macro):", f1_score(y_test, y_pred, average='macro'))
print("âœ… ROC-AUC (OvR):", roc_auc_score(y_test, y_proba, multi_class='ovr'))
# print("âœ… ROC-AUC:", roc_auc_score(y_test, y_proba[:, 1]))



print("\nğŸ“Š Classification Report:\n")
print(classification_report(y_test, y_pred))

# ==========================
# ğŸ“Š 11. ç‰¹å¾é‡è¦æ€§æå–
# ==========================

# 1ï¸âƒ£ æ‹¿åˆ°è®­ç»ƒå¥½çš„ LightGBM æ¨¡å‹
lgb_model = clf.named_steps["classifier"]

# 2ï¸âƒ£ è·å– OneHot ç¼–ç åçš„ç‰¹å¾å
# æ•°å€¼ç‰¹å¾åï¼ˆä¸ç”¨å˜ï¼‰
num_feature_names = numeric_features

# ç±»åˆ«ç‰¹å¾åï¼ˆéœ€è¦ä» onehot æ‹¿å‡ºæ¥ï¼‰
ohe_feature_names = clf.named_steps["preprocessor"].named_transformers_["cat"]["onehot"].get_feature_names_out(categorical_features)

# åˆå¹¶æˆå®Œæ•´ç‰¹å¾å
all_feature_names = np.concatenate([num_feature_names, ohe_feature_names])

# 3ï¸âƒ£ è·å–ç‰¹å¾é‡è¦æ€§
importances = lgb_model.feature_importances_

# 4ï¸âƒ£ æ„å»º DataFrame æ’åºå±•ç¤º
importance_df = pd.DataFrame({
    "Feature": all_feature_names,
    "Importance": importances
}).sort_values(by="Importance", ascending=False).reset_index(drop=True)

print("\nğŸŒŸ Top 20 Most Important Features:")
print(importance_df.head(20))
