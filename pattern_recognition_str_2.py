import numpy as np
import pandas as pd
import os, sys
from datetime import datetime
import matplotlib.pyplot as plt
import shutup
from tqdm import tqdm
from utils.data_process_toolkit import month_to_quarter
from utils.data_preparation_toolkit import situatuion_judgement, situatuion_judgement2
from API.api import get_price_from_csv, get_market_cap_from_csv
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report
from sklearn.utils.class_weight import compute_class_weight
import lightgbm as lgb

shutup.please()
# parameters
influence_period = 20 # trading days
lower_date_boundry = "2020-01-01" 
upper_date_boundry = "2025-05-01"
output_ols_report = True
num_total_quarter = 8
price_prev_window = 20
price_after_window = 20
random_seed = 42
num_of_parts = 1
beat_threshold = 0.1


PE_check_flag = False
Raw_data_process = False
beat_up_dataset_construction_strategy2 = False
pattern_recognition = True


path = "data/ern"
sector_df_path = "data/spx_sector.csv"
total_equity_df_prefix = "data/total_equity_df"
bbg_data_collect_path = "data/bbg_data_collect"
final_dataset_path = "data/final_dataset"
price_path = "data/price"

if PE_check_flag:
    for file in os.listdir(path):
        df = pd.read_excel(os.path.join(path, file), engine="openpyxl")
        columns = df.columns
        if "P/E" not in columns:
            print(f"[Error] {file} does not have P/E column")
            sys.exit(1)
        else:
            print(f"{file} check passed")

if Raw_data_process:
    total_equity_df_list = []
    sector_df = pd.read_csv(sector_df_path)
    sector_dic = dict(zip(sector_df["Ticker"], sector_df["Sector-Code"]))
    for equity in tqdm(os.listdir(path), desc="Raw Data Processing"):  
        columns = []
        equity_name = equity[:-5]
        data = pd.read_excel(os.path.join(path, equity), 
                            engine="openpyxl")
        data["Equity name"] = equity_name
        data["Ann Date"] = pd.to_datetime(data["Ann Date"], errors='coerce')
        data["Prev Ann Date"] = data["Ann Date"].shift(-1)
        data["Next Ann Date"] = data["Ann Date"].shift(1)

        data["EPS"] = data["Comp"]
        try:
            data["Surprise"] = data["%Surp"].replace("N.M.", "0").str.rstrip("%").astype(float) / 100
        except Exception:
            print(f"{equity_name} encountered a problem while transforming Surprise")
            sys.exit(1)

        for i in range(len(data)):
            pe = data.loc[i, "P/E"]
            if type(pe) is str:
                if 'k' in pe:
                    data.loc[i, "P/E"] = float(pe.strip("k")) * 1000
                elif pe == '':
                    pass
                else:
                    tqdm.write(f"{equity_name} has wrong value in PE")
                    sys.exit(1)
        data["PE"] = data["P/E"]
        data["PE Change"] = data["PE"].pct_change(periods=-1)

        data["%Px Chg"] = data["%Px Chg"].replace("N.M.", "0").str.rstrip("%").astype(float) / 100
        # Up: 1, Down: 0
        data["Up Down Flag"] = data["%Px Chg"].apply(lambda x: 1 if x > 0 else 0)
        # Beat: 1, Miss: 0
        data["Beat Miss Flag"] = data["Surprise"].apply(lambda x: 1 if x>0 else 0)

        try:
            data["Sector"] = sector_dic[equity_name]
        except KeyError as e:
            print(f"{equity_name} has no sector code")
            sys.exit(1)

        data["Quarter"] = data["Per End"].apply(month_to_quarter)

        columns.extend(["Equity name", "Ann Date", "Prev Ann Date", "Next Ann Date", "EPS", "Surprise", "PE", "PE Change", "Up Down Flag", "Beat Miss Flag", "Sector", "Quarter", "%Px Chg"])
        for i in range(num_total_quarter):
            col = f"Surprise {8-i}"
            data[col] = data["Surprise"].shift(8-i)
            columns.append(col)
        
        start_date_index = (data["Ann Date"] - pd.to_datetime(lower_date_boundry)).abs().idxmin()
        end_date_index = (data["Ann Date"] - pd.to_datetime(upper_date_boundry)).abs().idxmin()
        data = data.iloc[end_date_index:start_date_index,:]

        equity_df = data[columns]
        equity_df.dropna(how="any", inplace=True)
        if not equity_df.empty:
            total_equity_df_list.append(equity_df)

    total_equity_df = pd.concat(total_equity_df_list).reset_index(drop=True)
    total_equity_df = total_equity_df.sample(frac=1, random_state=42).reset_index(drop=True)
    total_equity_df = total_equity_df.reset_index(drop=True)
    print(f"number of total data is {len(total_equity_df)}")
    parts = np.array_split(total_equity_df, num_of_parts)
    for i, part in enumerate(parts):
        part.to_csv(f"{total_equity_df_prefix}_{i}.csv", index=False)

if beat_up_dataset_construction_strategy2:
    for file in os.listdir(bbg_data_collect_path):
        path = os.path.join(bbg_data_collect_path, file)
        total_equity_df= pd.read_csv(path)
        total_equity_df["total price seq"] = None
        total_equity_df["market cap"] = None
        total_equity_df["situation flag"] = None
        total_equity_df["price_prev"] = None
        # total_equity_df = total_equity_df[(total_equity_df["Beat Miss Flag"] ==1)&(total_equity_df["Up Down Flag"] == 1)&(total_equity_df["Surprise"] >=beat_threshold)].reset_index(drop=True)
        total_equity_df = total_equity_df[total_equity_df["Beat Miss Flag"] ==1].reset_index(drop=True)
        print(f"number of total data in {file} is {len(total_equity_df)}")
        for idx, row in tqdm(total_equity_df.iterrows(), 
                     total=total_equity_df.shape[0], 
                     desc=f"Beat Up Dataset Construction {file}"):
            equity_name = row["Equity name"]
            ann_date = row["Ann Date"]
            prev_ann_date = row["Prev Ann Date"]
            next_ann_date = row["Next Ann Date"]
            px_change = row["%Px Chg"]
            full_price_seq = get_price_from_csv(equity_name,prev_ann_date, next_ann_date, price_path).reset_index(drop=True)
            market_cap = get_market_cap_from_csv(equity_name, ann_date, price_path)
            try:
                day0_idx = full_price_seq.index[full_price_seq["Date"] == ann_date][0]
                day0_price = full_price_seq.loc[day0_idx, "Price"]
                day1_price = full_price_seq.loc[day0_idx+1, "Price"]
                day1_price_change = (day1_price - day0_price) / day0_price
                if abs(day1_price_change - px_change) > 0.001:
                    price_idx = day0_idx - 1
                else:
                    price_idx = day0_idx
                price_seq_prev = full_price_seq["Price"][price_idx+1-price_prev_window:price_idx+1].tolist()
                price_seq_after = full_price_seq[["Price"]][price_idx:price_idx+price_after_window+1]
            except Exception as e:
                total_equity_df.loc[idx, "situation flag"] = None
                total_equity_df.at[idx, "total price seq"] = None
                total_equity_df.loc[idx, "market cap"] = None
                total_equity_df.loc[idx, "price_prev"] = None
                continue

            if len(price_seq_prev) < price_prev_window or len(price_seq_after) < price_after_window:
                total_equity_df.loc[idx, "situation flag"] = None
                total_equity_df.at[idx, "total price seq"] = None
                total_equity_df.loc[idx, "market cap"] = None
                total_equity_df.loc[idx, "price_prev"] = None
                continue
            else:
                situation_flag, situation_details = situatuion_judgement2(price_seq_after, price_after_window)
                if situation_flag == 4:
                    print(equity_name, ann_date, prev_ann_date, next_ann_date, full_price_seq, price_seq_after.head(5))

            total_equity_df.loc[idx, "situation flag"] = situation_flag
            total_equity_df.at[idx, "total price seq"] = full_price_seq["Price"].tolist()
            total_equity_df.loc[idx, "market cap"] = market_cap
            total_equity_df.at[idx, "price_prev"] = price_seq_prev
        print(f"number of data befroe dropna: {len(total_equity_df)}")
        total_equity_df.dropna(subset="situation flag", inplace=True)
        print(f"number of validated in {file} is {len(total_equity_df)}")
        print(f"number of situation 1 is {len(total_equity_df[total_equity_df['situation flag']==1])}")
        print(f"number of situation 2 is {len(total_equity_df[total_equity_df['situation flag']==2])}")
        print(f"number of situation 3 is {len(total_equity_df[total_equity_df['situation flag']==3])}")
        total_equity_df.to_parquet(os.path.join(final_dataset_path, "beat_dataset", "beat_analysis_strategy_2"+".parquet"), engine="pyarrow", index=False)

if pattern_recognition:
    beat_up_dataset_path = "data/final_dataset/beat_dataset"
    prev_window_size = 5   # log return çª—å£é•¿åº¦ï¼ˆ11ä¸ªä»·æ ¼â†’10ä¸ªlog return

    beat_up_dataset = pd.read_parquet("data/final_dataset/beat_dataset/beat_analysis_strategy_2.parquet", engine="pyarrow")
    beat_up_dataset = beat_up_dataset[beat_up_dataset["situation flag"].isin([1,2])].reset_index(drop=True)
    print(f"âœ… number of total beat up data is {len(beat_up_dataset)}")

    y = beat_up_dataset["situation flag"].astype(int)

    def price_seq_to_log_returns(seq):
        prices = np.array(seq, dtype=np.float64)
        prices = prices[-prev_window_size:]
        returns = np.diff(np.log(prices))
        return returns
    
    log_return_expanded = beat_up_dataset["price_prev"].apply(price_seq_to_log_returns)
    log_return_df = pd.DataFrame(
        log_return_expanded.tolist(),
        columns=[f"log_return_day_{i}" for i in range(prev_window_size - 1)]
    )

    beat_up_dataset = pd.concat([beat_up_dataset.reset_index(drop=True), log_return_df], axis=1)

    for col in [f"Surprise {i}" for i in range(8, 0, -1)]:
        beat_up_dataset[col] = pd.to_numeric(beat_up_dataset[col], errors='coerce').fillna(0)

    numeric_features = [
        "EPS", "Surprise", "PE", "PE Change", "market cap", "%Px Chg"
    ] + [f"Surprise {i}" for i in range(8, 0, -1)] + [f"log_return_day_{i}" for i in range(prev_window_size - 1)]

    categorical_features = ["Sector", "Quarter"]
    feature_cols = numeric_features + categorical_features
    X = beat_up_dataset[feature_cols]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    classes = np.unique(y_train)
    weights = compute_class_weight(class_weight="balanced", classes=classes, y=y_train)
    class_weight_dict = {cls: w for cls, w in zip(classes, weights)}
    print("âœ… Computed class weights:", class_weight_dict)
    numeric_transformer = Pipeline(steps=[
        ("scaler", StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features)
        ]
    )

    model = lgb.LGBMClassifier(
        objective='multiclass',
        num_class=2,
        n_estimators=3000,          # âœ… å¢žåŠ è¿­ä»£æ¬¡æ•°ï¼Œè®©æ¨¡åž‹æœ‰æ›´å¤šå­¦ä¹ æœºä¼š
        learning_rate=0.02,         # âœ… é™ä½Žå­¦ä¹ çŽ‡ï¼Œè®©æ¯æ£µæ ‘å­¦å¾—æ›´ç»†
        num_leaves=31,             # âœ… å¢žåŠ å¶å­æ•°ï¼Œæå‡æ¨¡åž‹è¡¨è¾¾èƒ½åŠ›ï¼ˆé»˜è®¤31ï¼‰
        max_depth=-1,               # âœ… ä¸é™åˆ¶æ ‘æ·±åº¦ï¼Œè®©æ¨¡åž‹è‡ªç”±åˆ†è£‚
        min_child_samples=5,        # âœ… å‡å°‘åˆ†è£‚æ‰€éœ€æœ€å°æ ·æœ¬æ•°ï¼Œé¿å…æ—©åœ
        min_split_gain=0.0,         # âœ… æ”¾å®½åˆ†è£‚å¢žç›Šé˜ˆå€¼ï¼Œç¡®ä¿ä¸ä¼šå¤ªæ—©åœæ­¢
        subsample=0.8,              # âœ… éšæœºé‡‡æ ·ï¼Œæå‡æ³›åŒ–
        colsample_bytree=0.8,       # âœ… ç‰¹å¾å­é‡‡æ ·ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ
        reg_alpha=0.1,              # âœ… L1 æ­£åˆ™åŒ–ï¼ŒæŽ§åˆ¶å¤æ‚åº¦
        reg_lambda=0.1,             # âœ… L2 æ­£åˆ™åŒ–ï¼ŒæŽ§åˆ¶å¤æ‚åº¦
        class_weight=class_weight_dict,  # âœ… ä¿æŒç±»åˆ«æƒé‡
        random_state=42,
        force_col_wise=True         # âœ… å¤šåˆ†ç±»å°æ•°æ®å»ºè®®å¼€å¯ï¼Œæé«˜æ•ˆçŽ‡
    )

    clf = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("classifier", model)
    ])


    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    y_proba = clf.predict_proba(X_test)

    # ðŸ“Š æ‰“å°æ ‡ç­¾å æ¯”ï¼ˆçœŸå®žæ•°æ®åˆ†å¸ƒï¼‰
    print("\nðŸ“Š Label distribution (train set):")
    print(y_train.value_counts(normalize=True).sort_index().apply(lambda x: f"{x:.2%}"))

    print("\nðŸ“Š Label distribution (test set):")
    print(y_test.value_counts(normalize=True).sort_index().apply(lambda x: f"{x:.2%}"))


    # ðŸ“Š æ‰“å°æƒé‡å æ¯”ï¼ˆä¸ç”¨äºŽæ¨¡åž‹ï¼Œä»…åˆ†æžç”¨ï¼‰
    total_weight = sum(class_weight_dict.values())
    weight_ratio = {cls: w / total_weight for cls, w in class_weight_dict.items()}

    print("\nðŸ“Š Class weight ratios (not normalized for training, only for reference):")
    for cls, ratio in weight_ratio.items():
        print(f"  Class {cls}: {ratio:.2%}")


    print("âœ… Accuracy:", accuracy_score(y_test, y_pred))
    print("âœ… F1 Score (macro):", f1_score(y_test, y_pred, average='macro'))
    # print("âœ… ROC-AUC (OvR):", roc_auc_score(y_test, y_proba, multi_class='ovr'))
    print("âœ… ROC-AUC:", roc_auc_score(y_test, y_proba[:, 1]))



    print("\nðŸ“Š Classification Report:\n")
    print(classification_report(y_test, y_pred))

    # ==========================
    # ðŸ“Š 11. ç‰¹å¾é‡è¦æ€§æå–
    # ==========================

    # 1ï¸âƒ£ æ‹¿åˆ°è®­ç»ƒå¥½çš„ LightGBM æ¨¡åž‹
    lgb_model = clf.named_steps["classifier"]

    # 2ï¸âƒ£ èŽ·å– OneHot ç¼–ç åŽçš„ç‰¹å¾å
    # æ•°å€¼ç‰¹å¾åï¼ˆä¸ç”¨å˜ï¼‰
    num_feature_names = numeric_features

    # ç±»åˆ«ç‰¹å¾åï¼ˆéœ€è¦ä»Ž onehot æ‹¿å‡ºæ¥ï¼‰
    ohe_feature_names = clf.named_steps["preprocessor"].named_transformers_["cat"]["onehot"].get_feature_names_out(categorical_features)

    # åˆå¹¶æˆå®Œæ•´ç‰¹å¾å
    all_feature_names = np.concatenate([num_feature_names, ohe_feature_names])

    # 3ï¸âƒ£ èŽ·å–ç‰¹å¾é‡è¦æ€§
    importances = lgb_model.feature_importances_

    # 4ï¸âƒ£ æž„å»º DataFrame æŽ’åºå±•ç¤º
    importance_df = pd.DataFrame({
        "Feature": all_feature_names,
        "Importance": importances
    }).sort_values(by="Importance", ascending=False).reset_index(drop=True)

    print("\nðŸŒŸ Top 20 Most Important Features:")
    print(importance_df.head(20))
